{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "225ea311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# df_metrics = df_metrics.group_by(\"dataset\").agg(\n",
    "#     [\n",
    "#         (pl.col(col).mean().round(3).cast(pl.Utf8) + \" ± \" + \n",
    "#          pl.col(col).std().round(3).cast(pl.Utf8)).alias(col)\n",
    "#         for col in df_metrics.columns if col != \"dataset\"\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "def calculate_metrics_from_confusion_matrix(matrix_path):\n",
    "    \"\"\"\n",
    "    Calculates ACC, Sn, Sp, F1, and MCC from a binary confusion matrix CSV.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(matrix_path, index_col=0)\n",
    "\n",
    "    if len(df.columns) < 4:\n",
    "        # Extract values (assuming order like your example)\n",
    "        TN = df.loc[\"negative\", \"negative\"]\n",
    "        FP = df.loc[\"negative\", \"positive\"]\n",
    "        FN = df.loc[\"positive\", \"negative\"]\n",
    "        TP = df.loc[\"positive\", \"positive\"]\n",
    "\n",
    "        # Metrics\n",
    "        ACC = (TP + TN) / (TP + TN + FP + FN)\n",
    "        Sn = TP / (TP + FN) if (TP + FN) > 0 else 0  # Sensitivity (Recall)\n",
    "        Sp = TN / (TN + FP) if (TN + FP) > 0 else 0  # Specificity\n",
    "        F1 = (2 * TP) / (2 * TP + FP + FN) if (2 * TP + FP + FN) > 0 else 0\n",
    "        MCC = ((TP * TN) - (FP * FN)) / np.sqrt(\n",
    "            (TP + FP) * (TP + FN) * (TN + FP) * (TN + FN)\n",
    "        ) if all(x > 0 for x in [(TP + FP), (TP + FN), (TN + FP), (TN + FN)]) else 0\n",
    "\n",
    "        return {\n",
    "            \"ACC_test\": ACC,\n",
    "            \"Sn_test\": Sn,\n",
    "            \"Sp_test\": Sp,\n",
    "            \"F1_test\": F1,\n",
    "            \"MCC_test\": MCC,\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"ACC_test\": None,\n",
    "            \"Sn_test\": None,\n",
    "            \"Sp_test\": None,\n",
    "            \"F1_test\": None,\n",
    "            \"MCC_test\": None,\n",
    "        }\n",
    "\n",
    "# ---------- Main pipeline ----------\n",
    "pl.Config(tbl_rows=50)\n",
    "\n",
    "full_datasets_path = \"App/datasets\"\n",
    "datasets_list = [\n",
    "    os.path.join(full_datasets_path, item)\n",
    "    for item in os.listdir(full_datasets_path)\n",
    "    if os.path.isdir(os.path.join(full_datasets_path, item))\n",
    "]\n",
    "\n",
    "df_metrics = pd.DataFrame()\n",
    "\n",
    "for dataset_path in datasets_list:\n",
    "    experiments_folder = os.path.join(dataset_path, \"runs\")\n",
    "    if os.path.exists(experiments_folder):\n",
    "        runs_folders = [\n",
    "            os.path.join(experiments_folder, run_folder)\n",
    "            for run_folder in os.listdir(experiments_folder)\n",
    "        ]\n",
    "\n",
    "        for run_folder in runs_folders:\n",
    "            # --- Load training metrics ---\n",
    "            train_path = os.path.join(run_folder, \"training_kfold(10)_metrics.csv\")\n",
    "            if not os.path.exists(train_path):\n",
    "                continue\n",
    "\n",
    "            df_metrics_run = pd.read_csv(train_path)\n",
    "            df_metrics_run[\"dataset\"] = dataset_path.split(\"/\")[-1]\n",
    "\n",
    "            # --- Load test confusion matrix & compute metrics ---\n",
    "            test_confusion_path = os.path.join(run_folder, \"test_confusion_matrix.csv\")\n",
    "            test_other_path = os.path.join(run_folder, \"metrics_other.csv\")\n",
    "            if os.path.exists(test_other_path):\n",
    "                test_metrics = calculate_metrics_from_confusion_matrix(test_confusion_path)\n",
    "                for k, v in test_metrics.items():\n",
    "                    df_metrics_run[k] = v\n",
    "\n",
    "                df_metrics_run[\"AUC_test\"] = pl.read_csv(test_other_path).filter(pl.col(\"Metric\") == \"AUC\")[\"Value\"].item()\n",
    "            else:\n",
    "                # If missing, fill with NaN\n",
    "                for k in [\"ACC_test\", \"Sn_test\", \"Sp_test\", \"F1_test\", \"MCC_test\"]:\n",
    "                    df_metrics_run[k] = np.nan\n",
    "\n",
    "            df_metrics = pd.concat([df_metrics, df_metrics_run], ignore_index=True)\n",
    "\n",
    "# ---------- Sorting and Polars conversion ----------\n",
    "metric_columns = [\n",
    "    column for column in df_metrics.columns.drop([\"dataset\"]).tolist() if \"std\" not in column\n",
    "]\n",
    "df_metrics = pl.from_pandas(df_metrics[[\"dataset\"] + metric_columns]).sort(by=[\"dataset\"])\n",
    "\n",
    "# Optional sorting by dataset number if datasets are named like \"dataset1\", \"dataset2\", etc.\n",
    "df_sorted = (\n",
    "    df_metrics.with_columns(\n",
    "        pl.col(\"dataset\").str.extract(r\"dataset(\\d+)\").cast(pl.Int64).alias(\"dataset_num\")\n",
    "    )\n",
    "    .sort(\"dataset_num\")\n",
    "    .drop(\"dataset_num\")\n",
    ")\n",
    "\n",
    "df_sorted.select([\"dataset\", \"Sn\", \"Sp\", \"ACC\", \"MCC\", \"AUC\", \"Sn_test\", \"Sp_test\", \"ACC_test\", \"MCC_test\", \"AUC_test\"]).write_csv(\"runs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daed292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>dataset</th><th>ACC</th><th>MCC</th><th>F1</th><th>balanced_ACC</th><th>kappa</th><th>gmean</th><th>F1_micro</th><th>F1_macro</th><th>F1_w</th></tr><tr><td>str</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr></thead><tbody><tr><td>&quot;dataset1_zhang_protein&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 10)\n",
       "┌────────────────────────┬──────┬──────┬──────┬───┬───────┬──────────┬──────────┬──────┐\n",
       "│ dataset                ┆ ACC  ┆ MCC  ┆ F1   ┆ … ┆ gmean ┆ F1_micro ┆ F1_macro ┆ F1_w │\n",
       "│ ---                    ┆ ---  ┆ ---  ┆ ---  ┆   ┆ ---   ┆ ---      ┆ ---      ┆ ---  │\n",
       "│ str                    ┆ null ┆ null ┆ null ┆   ┆ null  ┆ null     ┆ null     ┆ null │\n",
       "╞════════════════════════╪══════╪══════╪══════╪═══╪═══════╪══════════╪══════════╪══════╡\n",
       "│ dataset1_zhang_protein ┆ null ┆ null ┆ null ┆ … ┆ null  ┆ null     ┆ null     ┆ null │\n",
       "└────────────────────────┴──────┴──────┴──────┴───┴───────┴──────────┴──────────┴──────┘"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_papers = pl.DataFrame({\"dataset\": [\"dataset1_zhang_protein\"], \n",
    "             \"ACC\": [0.871], \n",
    "             \"MCC\": [None], \n",
    "             \"F1\": [None], \n",
    "             \"balanced_ACC\": [None], \n",
    "             \"kappa\": [None], \n",
    "             \"gmean\": [None], \n",
    "             \"F1_micro\": [None], \n",
    "             \"F1_macro\": [None], \n",
    "             \"F1_w\": [None]})\n",
    "df_papers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
